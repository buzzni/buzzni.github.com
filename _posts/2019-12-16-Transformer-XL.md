---
layout: post
title: Transformer-XL
summary: Self-attention기반 model들의 문제점인 Context fragmentation를 해결하는 논문입니다.
tags: [NLP, language-model, self-attention, Transformer, context fragment]
author: Jeffrey
image: jeffrey
type: BUZZNI TECH SHARE
---


<iframe src="https://docs.google.com/presentation/d/1fc4kErxXcEKxbJtwwabhnP07zyahDn1Xy7WhKYvKQl8/preview" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>